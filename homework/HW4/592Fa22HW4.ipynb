{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM15h6gCxRF+Hja6TQivewk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/ATLAS/blob/main/homework/HW4/592Fa22HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 4\n",
        "## Homework guideline\n",
        "- The deadline is Nov 11th 10:30am. Submission after the deadline will not be graded. \n",
        "\n",
        "- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point. \n",
        "\n",
        "- Please justify all short answers with a brief explanation. It is highly recommended to use latex in the markdown. \n",
        "\n",
        "- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to. \n",
        "\n",
        "- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n",
        "\n",
        "- Feel free to use the lecture notes and other resources. But you\n",
        "must understand, write, and hand in your own answers. In addition, you must write and submit\n",
        "your own code in the programming part of the assignment (we may run your code).\n",
        "If you copy someone else homework solution, both of you may receive ZERO point. \n",
        "\n",
        "\n",
        "- Colab is preferred. However, if you use Anaconda, please download the .mat file locally and save it to the same folder as this homework file. "
      ],
      "metadata": {
        "id": "7Vd2p9ZDha-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collaboration:** List the names of all people you collaborated with and for which question(s). This is important!\n"
      ],
      "metadata": {
        "id": "s-fimMHhhwXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$"
      ],
      "metadata": {
        "id": "CGSSdt1wroEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pylab inline \n",
        "import numpy.linalg as LA\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "\n",
        "## Set some default values of the the matplotlib plots\n",
        "plt.rcParams['figure.figsize'] = (8.0, 8.0)  # Set default plot's sizes\n",
        "plt.rcParams['axes.grid'] = True  # Show grid by default in figures"
      ],
      "metadata": {
        "id": "xKIBIxLfiFzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1: Implementing Batch Naive Bayes (20pt)\n",
        "\n",
        "Implement a multinomial naive Bayes classifier in the `NaiveBayes` class. (You should know how to code python in Object-oriented programming.) Your implementation should support add-one smoothing. Whether or not to use add-one smoothing is controlled via an argument to the constructor; add-one smoothing is enabled by default.\n",
        "\n",
        "-  `__init(useAddOneSmoothing=True)__` : constructor.\n",
        "\n",
        "- `fit(X,Y)`: method to train the naive Bayes model.\n",
        "\n",
        "- `predict(X)`: method to use the trained naive Bayes model for prediction. \n",
        "\n",
        "- `predictProbs(X)`: outputs a matrix of predicted posterior class probabilities. \n",
        "\n",
        "\n",
        "The training data for multinomial naive Bayes is specified as feature counts: `X[i,j]` is the number of times\n",
        "feature $j$ occurs in instance $i$ (or you can think of it as that instance $i$ is characterized by a particular\n",
        "real-valued amount of feature $j$).\n",
        "\n",
        "We are here using the multinomial distribution. Suppose our dataset has $d$ features and $K$ classes. Then the\n",
        "multinomial distribution for a particular sample $(\\m{x},y)$, where $\\m{x}=[x_1,\\dots, x_D]\\in \\mb{R}^D$ and $y\\in \\{1,\\dots, K\\}$ is \n",
        "\\begin{align}\n",
        "p(\\m{x}|y) = \\frac{(\\sum_i x_i)! }{x_1! x_2! \\dots x_D!} p_{y1}^{x_1}p_{y2}^{x_2}\\dots p_{yD}^{x_D}\n",
        "\\end{align}\n",
        "and the label distribution $p(y)$ (in this case, a categorical distribution) is $\\mm\\pi=[\\pi_1, \\dots, \\pi_K]$. \n",
        "\n",
        "The naive Bayes assumption (that each instance is independent given the class label) is used in the sense that\n",
        "that we are assuming the generative process is\n",
        "\n",
        "- Picking a class $y$ according to the label distribution\n",
        "$p(y)$. \n",
        "\n",
        "- Generate a sequence of features, independently according to a multinomial distribution conditioned on the class $y$: $\\m{p}_y=(p_{y1}, p_{y2}, \\dots, p_{yD})$ with $\\sum_i p_{yi}=1$. \n",
        "\n",
        "What you are given is the count of each feature generated by this process stored in a vector $\\m{x}$. This is a useful model for predicting, say, document classes, where $d$ is size of vocabulary\n",
        "and $K$ is number of document classes. \n",
        "\n",
        "The MLE parameter estimation for this naive Bayes probabilistic model is\n",
        "- The label distribution $p(y)=\\hat{\\mm\\pi}$\n",
        "\\begin{align}\n",
        "\\hat{\\mm{\\pi}} = \\frac{N_c}{N}\n",
        "\\end{align}\n",
        "where $N_c$ is the number of samples with label $y=c$ and $N$ is the total samples. \n",
        "\n",
        "- The multinormial distribution with label $y=c$ is $\\hat{\\m{p}}_c=[\\hat{p}_{c1}, \\dots, \\hat{p}_{cD}]$ and \n",
        "\\begin{align}\n",
        "\\hat{p}_{ci}  =\\frac{N_{ci}}{N_c} \n",
        "\\end{align}\n",
        "where $N_{ci}$ is total occurrences of feature $i$ in samples with label $y=c$. \n",
        "\n",
        "\n",
        "- When using add-one smoothing, we estimate $\\hat{p}_{ci} $ with \n",
        "\\begin{align}\n",
        "\\hat{p}_{ci}  = \\frac{N_{ci}+1}{N_c +D}\n",
        "\\end{align}\n",
        "\n",
        "During prediction, given feature count vector $\\m{x}$, we estimate label $y$ posterior probability with\n",
        "\\begin{align}\n",
        "p(y|\\m{x})\\propto p(y)\\hat{p}_{y1}^{x_1}p_{y2}^{x_2}\\dots p_{yD}^{x_D}\n",
        "\\end{align}\n",
        "up to normalization. You might want to implement the equation above with summation of log probabilities\n",
        "for better numerical stability. If you choose to do so, you would need another **numerical trick** below. \n",
        "\n",
        "After\n",
        "obtaining the log probabilities for each classes, $\\m{z}=\\bcm\\log p(y=1|\\m{x}),\\log p(y=2|\\m{x}), \\dots, \\log p(y=K|\\m{x}) \\ecm$. The actual probability distribution to be output is\n",
        "\\begin{align}\n",
        "p(y=c|\\m{x}) &=\\frac{\\exp(z_c)}{\\sum_{c=1}^K \\exp(z_c)}\\\\\n",
        "&= \\frac{\\exp(z_c-z)}{\\sum_{c=1}^K \\exp(z_c-z)}\n",
        "\\end{align}\n",
        "where $z=\\max_c z_c$. Here we subtract $\\max_c z_c$ from the log probabilities for better numerical stability. \n",
        "\n",
        "The `predictProbs(X)` function takes in a matrix $X$ of $N$ instances and outputs an $N\\times K$ matrix of posterior\n",
        "probabilities. Each row $i$ of the returned matrix represents the posterior probability distribution over the $K$\n",
        "classes for the $i$-th training instance. (Note that each row of the returned matrix will sum to 1.)"
      ],
      "metadata": {
        "id": "EevE2gqOCyfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayes:\n",
        "\n",
        "    def __init__(self, useAddOneSmoothing=True):\n",
        "        \"\"\"\n",
        "        Constructor\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "            y is an n-dimensional numpy array\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Hint: np.unique and np logical functions (logical_and/or/not) may be helpful to your implementation\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "        Returns:\n",
        "            an n-dimensional numpy array of the predictions\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "\n",
        "    def predictProbs(self, X):\n",
        "        \"\"\"\n",
        "        Used the model to predict a vector of class probabilities for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy array\n",
        "        Returns:\n",
        "            an n-by-K numpy array of the predicted class probabilities (for K classes)\n",
        "        \"\"\"\n",
        "        # TODO"
      ],
      "metadata": {
        "id": "f1VlcOYq_c9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# load the data set\n",
        "dataset = datasets.load_digits()\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "n, d = X.shape\n",
        "nTrain = int(0.5*n)  # training on 50% of the data\n",
        "\n",
        "# shuffle the data\n",
        "idx = np.arange(n)\n",
        "np.random.seed(13)\n",
        "np.random.shuffle(idx)\n",
        "X = X[idx]\n",
        "y = y[idx]\n",
        "\n",
        "# split the data\n",
        "Xtrain = X[:nTrain, :]\n",
        "ytrain = y[:nTrain]\n",
        "Xtest = X[nTrain:, :]\n",
        "ytest = y[nTrain:]\n",
        "\n",
        "\n",
        "# train the naive Bayes\n",
        "modelNB = NaiveBayes(useAddOneSmoothing=True)\n",
        "modelNB.fit(Xtrain, ytrain)\n",
        "\n",
        "# output predictions on the remaining data\n",
        "ypred_NB = modelNB.predict(Xtest)\n",
        "\n",
        "# calculate the posterior probability \n",
        "yposterior_NB = modelNB.predictProbs(Xtest) \n",
        "\n",
        "# compute the training accuracy of the model\n",
        "accuracyNB = accuracy_score(ytest, ypred_NB)\n",
        "\n",
        "print(\"Naive Bayes Accuracy = \" + str(accuracyNB))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NN3rUcQ_AZGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Description\n",
        "In next three questions, we will use the various methods for face recognition. Our task here is to be able to predict the correct label (name of the person) given an image of his face.\n",
        "\n",
        "We will use the same dataset in the last homework. In these problem, we are not allowed to use pyTorch yet!"
      ],
      "metadata": {
        "id": "mAfdl4yEiLWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "dataset = fetch_lfw_people(min_faces_per_person=100)\n",
        "\n",
        "X = dataset.images\n",
        "y = dataset.target\n",
        "label_to_name_mapping = dataset.target_names\n",
        "image_shape = X[0].shape\n",
        "\n",
        "print('Number of images in the dataset: {}'.format(len(X)))\n",
        "print('Number of different persons in the dataset: {}'.format(len(np.unique(y))))\n",
        "print('Each images size is: {}'.format(image_shape))\n",
        "\n",
        "_, images_per_class = np.unique(y, return_counts=True)\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(label_to_name_mapping, images_per_class)\n",
        "ax.set_xticklabels(label_to_name_mapping, rotation=-90);\n",
        "ax.set_title('Images per person')\n",
        "ax.set_ylabel('Number of images')\n",
        "\n",
        "\n",
        "# plots the first 20 images in the dataset. \n",
        "fig, ax_array = plt.subplots(3, 3)\n",
        "for i, ax in enumerate(ax_array.flat):\n",
        "    ax.imshow(X[i], cmap='gray')\n",
        "    ax.set_ylabel(label_to_name_mapping[y[i]])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xticks([])"
      ],
      "metadata": {
        "id": "WDxa4NA6hQqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split the data into 80% training set and 20% testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "AOhdOiO1hTtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Q2: Logistic regression with Softmax (30pt)\n",
        "Let's do multi-class classification with the softmax function. \n",
        "Remember the model is as follows: \n",
        "\n",
        "1. the probability for $K$ classes are  \n",
        "\\begin{align}\n",
        "p(y=c |\\mathbf{x}, \\mathbf{W}) = \\frac{\\exp(\\mathbf{x}\\cdot \\mathbf{w}_c )}{\\sum_{j=0}^{K-1} \\exp(\\mathbf{x} \\cdot\\mathbf{w}_j ) }\n",
        "\\end{align}\n",
        "where $\\mathbf{w}_c $ is $c$-th column of $\\mathbf{W}$. Here we ignore the bias vector. \n",
        "\n",
        "2. In prediction, you will take the largest predicted probability among your $K$ predicted probability. \n",
        "\n",
        "3. The negative log-likelihood function on the $N$ training dataset is \n",
        "\\begin{align}\n",
        "\\text{NLL}(\\mathbf{W}) = -\\frac{1}{N}\\sum_{i=1}^N \\log p(y=y^{(i)} | \\mathbf{x}^{(i)}, \\mathbf{W})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.1 Derivative (10pt)\n",
        "Express the label $y$ into one hot vector, i.e., $y=c$ is represented by $\\vec{y}$ with $\\vec{y}_c =1$ and other indices are zero. \n",
        "\n",
        "Calculate the derivative of the negative log-likelihood with respect to the variable $\\mathbf{W}$. Write this in a natural manner in terms of conditional probability, the data matrix $\\mathbf{X}$. Don't write this expression in terms of exponentials explicitly. \n",
        "You may refer to results in previous lectures. \n"
      ],
      "metadata": {
        "id": "Not_3zS-ynsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "n0tYucQk_MEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.2 Implementation (20pt)\n",
        "(a) Implement softmax classification with SGD. Specify your learning rate (or your learning rate decay scheme if you alter learning rate). \n",
        "\n",
        "(b) For both training dataset and testing dataset, plot the negative log-likelihood as function of the epoch number. \n",
        "\n",
        "(c) For both training dataset and testing dataset, plot the misclassification rate as function of the epoch number.\n",
        "\n",
        "(d) How many iterations did this take you? \n",
        "\n"
      ],
      "metadata": {
        "id": "fKKECmhU_zwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.2: your code starts here. \n"
      ],
      "metadata": {
        "id": "iGuERadPBZrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "sgiMYEM3BcPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2.3 (optional) Compare with full batch gradient descent \n",
        "\n",
        "(a) Implement the previous algorithm with full batch gradient descent. What learning rate did you use? \n",
        "\n",
        "(b) Please plot the similar plots as you did in the previous question. \n",
        "\n",
        "(c) At last, compare the computational complexity it took you to reach a comparable misclassification rate on your training set. \n",
        "\n",
        "(d) Do you think SGD performs better than full batch gradient descent or not? "
      ],
      "metadata": {
        "id": "B-p5VEKIBjb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2.3: your code starts here. \n"
      ],
      "metadata": {
        "id": "CaKfVDC3CeqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer: "
      ],
      "metadata": {
        "id": "q0mtjE9YCgJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q3: Deep neural network with Sigmoid function and squared loss (20pt)\n",
        "In class, we showed how to implement DNN with the sigmoid function as the activation function. In particular, the output layer also uses the sigmoid function. \n",
        "\n",
        "(a) Please setup your own deep neural network to classify these faces. Here you can use sigmoid function as activation function and output function, and you can use the square loss function as well. But you have to choose the number of the layer and hidden neurons by yourself. **Happy tuning!**\n",
        "\n",
        "(b) For both training dataset and testing dataset, plot the cost as function of the epoch number. \n",
        "\n",
        "(c) For both training dataset and testing dataset, plot the misclassification rate as function of the epoch number.\n",
        "\n",
        "(d) Did DNN performs better than logistic regression or not? \n"
      ],
      "metadata": {
        "id": "hg0yA3JWzkbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3: your code starts here. \n"
      ],
      "metadata": {
        "id": "peqcx-7gz6kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "uX3lCBxVDqp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Q4: Deep neural network with Relu function and squared loss (30pt)\n",
        "Another popular choice of the activation function is relu function. \n",
        "\n",
        "(a) Modify your code to use Relu function in these hidden nueron layer and sigmoid/softmax function in the output layer. You may try  1 hidden layer of 1024 neurons in this case.  i.e., \n",
        "\n",
        "- A fully connected (linear) layer with an input of the n_features and output of 1024.\n",
        "- A ReLU layer\n",
        "- A fully connected (linear) layer with an input of 1024 and output of n_classes.\n",
        "- A softmax function.\n",
        "\n",
        "\n",
        "(b) Compare the performance with Q2 and Q3. "
      ],
      "metadata": {
        "id": "QQFRzZ7pGZ_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: your code starts here. \n"
      ],
      "metadata": {
        "id": "ZDbdEAeYGidt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "_2MgjykeGkTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Q5: (optional) Deep neural network with logistic loss \n",
        "\n",
        "(a) What about switching to logistic loss? Do you find it is nearly impossible to tune here due to the vanishing gradient and other errors/warnings? \n",
        "\n",
        "(b) Please dig out the root cause and fix it! You should get similar performance as the square loss. \n"
      ],
      "metadata": {
        "id": "F2SGDdzn0C1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: your code starts here. \n"
      ],
      "metadata": {
        "id": "2PT3VfnT1cxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Answer:"
      ],
      "metadata": {
        "id": "h3Uw_KWuDnbV"
      }
    }
  ]
}